{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cognitive.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5TMExQlWpX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRzMnoWbEY3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create validation set\n",
        "img_list = pd.read_csv(os.getcwd()+'/data/driver_imgs_list.csv')\n",
        "valid_subjects = img_list.subject.sort_values().unique()[-6:]\n",
        "img_list['is_valid'] = img_list['subject'].isin(valid_subjects)\n",
        "print(\"valid subjects: \", valid_subjects)\n",
        "print(img_list[img_list['is_valid']==True].subject.count())\n",
        "img_list['img_path'] = img_list.classname + '/' + img_list.img\n",
        "valid_names = img_list[img_list['subject'].isin(valid_subjects)].img\n",
        "valid_names = valid_names.to_list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udFIvPa1Svq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data loading\n",
        "data = (ImageList.from_df(df=img_list, path = 'data/imgs/train/', cols='img_path')\n",
        "        .split_by_valid_func(lambda o: os.path.basename(o) in valid_names)\n",
        "        #.split_by_rand_pct(.2)\n",
        "        .label_from_df(1)\n",
        "        .transform(tfms=get_transforms(do_flip=False), size=224)\n",
        "        .add_test_folder('../test')\n",
        "        .databunch(bs=64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1svCYfXtz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.torch_core import *\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch,math,sys\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from functools import partial\n",
        "\n",
        "__all__ = ['XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqRchSv_dcxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.script import *\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks import *\n",
        "from fastai.distributed import *\n",
        "from fastprogress import fastprogress\n",
        "from torchvision.models import *\n",
        "from functools import partial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYxt-sKHX_ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act_fn = nn.ReLU(inplace=True)\n",
        "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
        "  \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
        "  conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
        "  nn.init.kaiming_normal_(conv.weight)\n",
        "  if bias: conv.bias.data.zero_()\n",
        "  return spectral_norm(conv)\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "  def __init__(self, n_in:int, ks=1, sym=False):#, n_out:int):\n",
        "    super().__init__()      \n",
        "    self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
        "    self.gamma = nn.Parameter(tensor([0.]))    \n",
        "    self.sym = sym\n",
        "    self.n_in = n_in\n",
        "\n",
        "  def forward(self,x):      \n",
        "    if self.sym:\n",
        "      # symmetry hack by https://github.com/mgrankin\n",
        "      c = self.conv.weight.view(self.n_in,self.n_in)\n",
        "      c = (c + c.t())/2\n",
        "      self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
        "\n",
        "    size = x.size()  \n",
        "    x = x.view(*size[:2],-1)   # (C,N)\n",
        "      \n",
        "    # changed the order of mutiplication to avoid O(N^2) complexity\n",
        "    # (x*xT)*(W*x) instead of (x*(xT*(W*x)))\n",
        "    convx = self.conv(x)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
        "    xxT = torch.bmm(x,x.permute(0,2,1).contiguous())   # (C,N) * (N,C) = (C,C)   => O(NC^2)\n",
        "    o = torch.bmm(xxT, convx)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
        "    o = self.gamma * o + x\n",
        "    return o.view(*size).contiguous()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3X5CcZqdTnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x): return x.view(x.size(0), -1)\n",
        "\n",
        "def init_cnn(m):\n",
        "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
        "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
        "    for l in m.children(): init_cnn(l)\n",
        "\n",
        "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
        "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
        "\n",
        "def noop(x): return x\n",
        "\n",
        "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
        "    bn = nn.BatchNorm2d(nf)\n",
        "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
        "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
        "    if act: layers.append(act_fn)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, expansion, ni, nh, stride=1,sa=False, sym=False):\n",
        "        super().__init__()\n",
        "        nf,ni = nh*expansion,ni*expansion\n",
        "        layers  = [conv_layer(ni, nh, 3, stride=stride),\n",
        "                   conv_layer(nh, nf, 3, zero_bn=True, act=False)\n",
        "        ] if expansion == 1 else [\n",
        "                   conv_layer(ni, nh, 1),\n",
        "                   conv_layer(nh, nh, 3, stride=stride),\n",
        "                   conv_layer(nh, nf, 1, zero_bn=True, act=False)\n",
        "        ]\n",
        "        \n",
        "        self.sa = SimpleSelfAttention(nf,ks=1,sym=sym) if sa else noop\n",
        "        \n",
        "        self.convs = nn.Sequential(*layers)\n",
        "        # TODO: check whether act=True works better\n",
        "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n",
        "        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
        "\n",
        "    def forward(self, x): return act_fn(self.sa(self.convs(x)) + self.idconv(self.pool(x)))\n",
        "\n",
        "def filt_sz(recep): return min(64, 2**math.floor(math.log2(recep*0.75)))\n",
        "    \n",
        "class XResNet(nn.Sequential):\n",
        "    def __init__(self, expansion, layers, c_in=3, c_out=1000, sa = False, sym= False):\n",
        "        \n",
        "        \n",
        "        stem = []\n",
        "        sizes = [c_in,32,64,64]\n",
        "        for i in range(3):\n",
        "            stem.append(conv_layer(sizes[i], sizes[i+1], stride=2 if i==0 else 1))\n",
        "            #nf = filt_sz(c_in*9)\n",
        "            #stem.append(conv_layer(c_in, nf, stride=2 if i==1 else 1))\n",
        "            #c_in = nf\n",
        "\n",
        "        block_szs = [64//expansion,64,128,256,512]\n",
        "        blocks = [self._make_layer(expansion, block_szs[i], block_szs[i+1], l, 1 if i==0 else 2, sa = sa if i in[len(layers)-2] else False, sym=sym)\n",
        "                  for i,l in enumerate(layers)]\n",
        "        super().__init__(\n",
        "            *stem,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            *blocks,\n",
        "            nn.AdaptiveAvgPool2d(1), \n",
        "            Flatten(),\n",
        "            nn.BatchNorm1d(block_szs[-1]*expansion, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            #nn.Dropout(p=0.15, inplace=False),\n",
        "            #nn.Linear(block_szs[-1]*expansion, int(block_szs[-1]*expansion/4)),\n",
        "            nn.Linear(block_szs[-1]*expansion, c_out, bias=True),\n",
        "            #nn.ReLU(inplace=True),\n",
        "            #nn.BatchNorm1d(int(block_szs[-1]*expansion/4), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            #nn.Dropout(p=0.5, inplace=False),\n",
        "            #nn.Linear(in_features=int(block_szs[-1]*expansion/4), out_features=c_out,bias=True),\n",
        "            #nn.Softmax(),\n",
        "        )\n",
        "        init_cnn(self)\n",
        "\n",
        "    def _make_layer(self, expansion, ni, nf, blocks, stride, sa=False, sym=False):\n",
        "        return nn.Sequential(\n",
        "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1, sa if i in [blocks -1] else False,sym)\n",
        "              for i in range(blocks)])\n",
        "\n",
        "def xresnet(expansion, n_layers, name, pretrained=False,  **kwargs):\n",
        "    model = XResNet(expansion, n_layers, **kwargs)\n",
        "    if pretrained: \n",
        "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'))\n",
        "    return model\n",
        "\n",
        "me = sys.modules[__name__]\n",
        "for n,e,l in [\n",
        "    [ 18 , 1, [2,2,2 ,2] ],\n",
        "    [ 34 , 1, [3,4,6 ,3] ],\n",
        "    [ 50 , 4, [3,4,6 ,3] ],\n",
        "    [ 101, 4, [3,4,23,3] ],\n",
        "    [ 152, 4, [3,8,36,3] ],\n",
        "]:\n",
        "    name = f'xresnet{n}'\n",
        "    setattr(me, name, partial(xresnet, expansion=e, n_layers=l, name=name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKac5W4jdmQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "fastprogress.MAX_COLS = 80\n",
        "self_attention = 1\n",
        "symmetry = 0\n",
        "mom = 0.9 # parameter momentum\n",
        "alpha = 0.99 # parameter alpha\n",
        "eps = 1e-6 # parameter epsilon\n",
        "opt = 'adam' # 'rms', 'sgd'\n",
        "if opt=='adam': opt_func = partial(optim.Adam, betas=(mom,alpha), eps=eps)\n",
        "elif opt=='rms': opt_func = partial(optim.RMSprop, alpha=alpha, eps=eps)\n",
        "elif opt=='sgd': opt_func = partial(optim.SGD, momentum=mom)\n",
        "#log_cb = partial(CSVLogger,filename=log)\n",
        "m = globals()['xresnet50']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJTv2aZ4doeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = (Learner(data, m(c_out=10, sa=self_attention, sym=symmetry), wd=1e-2, opt_func=opt_func,\n",
        "             metrics=[accuracy, error_rate, FBeta(average='macro')],\n",
        "             bn_wd=False, true_wd=True,\n",
        "             loss_func = LabelSmoothingCrossEntropy()\n",
        "             ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8qChWt9d5Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3 # learning rate\n",
        "epochs = 5\n",
        "learn = learn.to_fp16(dynamic=True)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(epochs, max_lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ethh6Bp-SFMu",
        "colab_type": "code",
        "outputId": "abc2f110-5c09-4a13-dff8-f511289aa7d7",
        "colab": {}
      },
      "source": [
        "# The best result we have for the validation set.\n",
        "learn.fit_one_cycle(1, max_lr=1e-8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>f_beta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.504300</td>\n",
              "      <td>1.012736</td>\n",
              "      <td>0.803917</td>\n",
              "      <td>0.196083</td>\n",
              "      <td>0.805633</td>\n",
              "      <td>02:33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}